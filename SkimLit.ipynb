{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "582cd8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries.....\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score,precision_recall_fscore_support\n",
    "from tensorflow.keras.layers import TextVectorization,Embedding\n",
    "import warnings\n",
    "import random\n",
    "from tensorflow.keras import layers\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "880ca0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter warnings.\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b691002a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for downloading data.....\n",
    "# !git clone https://github.com/Franck-Dernoncourt/pubmed-rct.git\n",
    "# !ls pubmed-rct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b15482",
   "metadata": {},
   "source": [
    "The goal of this model is to classify the sentences which appear in the sequences, like what role each sentences serve in the abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f288b4de",
   "metadata": {},
   "source": [
    "## What we're going to do:\n",
    "    ~ Download Dataset\n",
    "    ~ Writing Preprocessing Function to prepare data for modelling\n",
    "    ~ Setup Series of Model Experiment\n",
    "    ~ Make MultiModel Model\n",
    "    ~ Make Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b458443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out the files in the dataset\n",
    "# !ls pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a059aa",
   "metadata": {},
   "source": [
    "Well we've three files there first one is for development set which is another name for validation data while 2nd and 3rd files contain data for testing and training respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f38c2b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check all filenames in the directory....\n",
    "# data_dir = \"pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/\"\n",
    "# filenames = [data_dir + fname for fname in os.listdir(data_dir)]\n",
    "# filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d53d1738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md     dev.txt       test.txt\r\n",
      "SkimLit.ipynb \u001b[34mpubmed-rct\u001b[m\u001b[m    train.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d041b2be",
   "metadata": {},
   "source": [
    "## Preprocessing Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "defd92ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make a function that will preprocess the data\n",
    "def get_lines(fname):\n",
    "    \"\"\"\n",
    "    reads the filename and return the lines of the text as a list\n",
    "    \"\"\"\n",
    "    with open(fname,\"r\") as file:\n",
    "        return file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3217ea7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md     dev.txt       test.txt\r\n",
      "SkimLit.ipynb \u001b[34mpubmed-rct\u001b[m\u001b[m    train.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9465d9ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['###24293578\\n',\n",
       " 'OBJECTIVE\\tTo investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( OA ) .\\n',\n",
       " 'METHODS\\tA total of @ patients with primary knee OA were randomized @:@ ; @ received @ mg/day of prednisolone and @ received placebo for @ weeks .\\n',\n",
       " 'METHODS\\tOutcome measures included pain reduction and improvement in function scores and systemic inflammation markers .\\n',\n",
       " 'METHODS\\tPain was assessed using the visual analog pain scale ( @-@ mm ) .\\n']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the lines \n",
    "train_lines = get_lines(\"train.txt\")\n",
    "train_lines[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d55ffe70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['###24293578\\n',\n",
       " 'OBJECTIVE\\tTo investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( OA ) .\\n']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lines[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89fc85c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(filename):\n",
    "  input_lines = get_lines(filename) # get all lines from filename\n",
    "  abstract_lines = \"\" # create an empty abstract\n",
    "  abstract_samples = [] # create an empty list of abstracts\n",
    "  \n",
    "  # Loop through each line in target file\n",
    "  for line in input_lines:\n",
    "    if line.startswith(\"###\"): # check to see if line is an ID line\n",
    "      abstract_id = line\n",
    "      abstract_lines = \"\" # reset abstract string\n",
    "    elif line.isspace(): # check to see if line is a new line\n",
    "      abstract_line_split = abstract_lines.splitlines() # split abstract into separate lines\n",
    "\n",
    "      # Iterate through each line in abstract and count them at the same time\n",
    "      for abstract_line_number, abstract_line in enumerate(abstract_line_split):\n",
    "        line_data = {} # create empty dict to store data from line\n",
    "        target_text_split = abstract_line.split(\"\\t\") # split target label from text\n",
    "        line_data[\"target\"] = target_text_split[0] # get target label\n",
    "        line_data[\"text\"] = target_text_split[1].lower() # get target text and lower it\n",
    "        line_data[\"line_number\"] = abstract_line_number # what number line does the line appear in the abstract?\n",
    "        line_data[\"total_lines\"] = len(abstract_line_split) - 1 # how many total lines are in the abstract? (start from 0)\n",
    "        abstract_samples.append(line_data) # add line data to abstract samples list\n",
    "    \n",
    "    else: # if the above conditions aren't fulfilled, the line contains a labelled sentence\n",
    "      abstract_lines += line\n",
    "  \n",
    "  return abstract_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31baaed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180040, 30212, 30135)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the data from files and preprocess it\n",
    "train_samples = preprocess_text(\"train.txt\")\n",
    "val_samples = preprocess_text(\"dev.txt\")\n",
    "test_samples = preprocess_text(\"test.txt\")\n",
    "\n",
    "# checking the length of each sample\n",
    "len(train_samples),len(val_samples),len(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40ccdbf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>line_number</th>\n",
       "      <th>total_lines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OBJECTIVE</td>\n",
       "      <td>to investigate the efficacy of @ weeks of dail...</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>a total of @ patients with primary knee oa wer...</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>outcome measures included pain reduction and i...</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>pain was assessed using the visual analog pain...</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>secondary outcome measures included the wester...</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180035</th>\n",
       "      <td>RESULTS</td>\n",
       "      <td>for the absolute change in percent atheroma vo...</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180036</th>\n",
       "      <td>RESULTS</td>\n",
       "      <td>for pav , a significantly greater percentage o...</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180037</th>\n",
       "      <td>RESULTS</td>\n",
       "      <td>both strategies had acceptable side effect pro...</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180038</th>\n",
       "      <td>CONCLUSIONS</td>\n",
       "      <td>compared with standard statin monotherapy , th...</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180039</th>\n",
       "      <td>CONCLUSIONS</td>\n",
       "      <td>( plaque regression with cholesterol absorptio...</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180040 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             target                                               text  \\\n",
       "0         OBJECTIVE  to investigate the efficacy of @ weeks of dail...   \n",
       "1           METHODS  a total of @ patients with primary knee oa wer...   \n",
       "2           METHODS  outcome measures included pain reduction and i...   \n",
       "3           METHODS  pain was assessed using the visual analog pain...   \n",
       "4           METHODS  secondary outcome measures included the wester...   \n",
       "...             ...                                                ...   \n",
       "180035      RESULTS  for the absolute change in percent atheroma vo...   \n",
       "180036      RESULTS  for pav , a significantly greater percentage o...   \n",
       "180037      RESULTS  both strategies had acceptable side effect pro...   \n",
       "180038  CONCLUSIONS  compared with standard statin monotherapy , th...   \n",
       "180039  CONCLUSIONS  ( plaque regression with cholesterol absorptio...   \n",
       "\n",
       "        line_number  total_lines  \n",
       "0                 0           11  \n",
       "1                 1           11  \n",
       "2                 2           11  \n",
       "3                 3           11  \n",
       "4                 4           11  \n",
       "...             ...          ...  \n",
       "180035            7           11  \n",
       "180036            8           11  \n",
       "180037            9           11  \n",
       "180038           10           11  \n",
       "180039           11           11  \n",
       "\n",
       "[180040 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert them into pandas data-frame\n",
    "train_df = pd.DataFrame(train_samples)\n",
    "val_df = pd.DataFrame(val_samples)\n",
    "test_df = pd.DataFrame(test_samples)\n",
    "\n",
    "# check out the head of train df\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6c074c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "METHODS        59353\n",
       "RESULTS        57953\n",
       "CONCLUSIONS    27168\n",
       "BACKGROUND     21727\n",
       "OBJECTIVE      13839\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the destribution of the target values...\n",
    "train_df[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d55762c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's visulize it...\n",
    "train_df[\"target\"].value_counts().plot(kind=\"bar\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3ff79a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['to investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( oa ) .',\n",
       " 'a total of @ patients with primary knee oa were randomized @:@ ; @ received @ mg/day of prednisolone and @ received placebo for @ weeks .',\n",
       " 'outcome measures included pain reduction and improvement in function scores and systemic inflammation markers .',\n",
       " 'pain was assessed using the visual analog pain scale ( @-@ mm ) .',\n",
       " 'secondary outcome measures included the western ontario and mcmaster universities osteoarthritis index scores , patient global assessment ( pga ) of the severity of knee oa , and @-min walk distance ( @mwd ) .']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the sentences from data-frame line X values\n",
    "train_sentences = train_df[\"text\"].tolist()\n",
    "val_sentences = val_df[\"text\"].tolist()\n",
    "test_sentences = test_df[\"text\"].tolist()\n",
    "\n",
    "# viewing couple of few lines...\n",
    "train_sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81afcb1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 2, ..., 4, 1, 1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the \"Target\" Y values and convert them into One-Hot-Encoder\n",
    "encoder = LabelEncoder()\n",
    "train_labels = encoder.fit_transform(train_df[\"target\"].to_numpy())\n",
    "test_labels = encoder.transform(test_df[\"target\"].to_numpy())\n",
    "val_labels = encoder.transform(val_df[\"target\"].to_numpy())\n",
    "\n",
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "60429a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BACKGROUND' 'CONCLUSIONS' 'METHODS' 'OBJECTIVE' 'RESULTS']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(encoder.classes_)\n",
    "num_classes = len(encoder.classes_)\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5d7fa3",
   "metadata": {},
   "source": [
    "## Model Experiments:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a577aa3b",
   "metadata": {},
   "source": [
    "## Model 0 : BaseLine Model\n",
    "\n",
    "our first baseline model will be scikit-learn Naive Bayes Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c31309c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()), (&#x27;model&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()), (&#x27;model&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()), ('model', MultinomialNB())])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# baseline model\n",
    "model_0 = Pipeline([\n",
    "    (\"tfidf\",TfidfVectorizer()),\n",
    "    (\"model\",MultinomialNB())\n",
    "])\n",
    "\n",
    "# fitting the model on data....\n",
    "model_0.fit(train_sentences,train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d53827a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72.1832384482987"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate baseline model...\n",
    "model_0.score(val_sentences,val_labels) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27808094",
   "metadata": {},
   "source": [
    "## Model Perfomance Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f992871c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make a function that will evaluate our model performance\n",
    "def calculate_results(y_true,y_pred):\n",
    "    \"\"\"\n",
    "    This function will take two parameters and will return the Model Accuracy,Precision,Recall and Support.\n",
    "    \"\"\"\n",
    "    model_accuracy = accuracy_score(y_true,y_pred) * 100\n",
    "    model_precision,model_recall,model_fscore,_ = precision_recall_fscore_support(y_true,y_pred,\n",
    "                                                                                  average=\"weighted\")\n",
    "    results = {\"Accuracy\" : model_accuracy,\n",
    "              \"Precision\" : model_precision,\n",
    "              \"Recall\" : model_recall,\n",
    "              \"Fscore\" : model_fscore}\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa95bccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 1, 3, ..., 4, 4, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make prediction on baseline model\n",
    "baseline_pred = model_0.predict(val_sentences)\n",
    "baseline_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10e9d98d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Accuracy': 72.1832384482987,\n",
       " 'Precision': 0.7186466952323352,\n",
       " 'Recall': 0.7218323844829869,\n",
       " 'Fscore': 0.6989250353450294}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's calculate the results...\n",
    "baseline_results = calculate_results(val_labels,baseline_pred)\n",
    "baseline_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86caa9ce",
   "metadata": {},
   "source": [
    "Wow! look like our model has achieved accuracy of 72.1% not bad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e15be74",
   "metadata": {},
   "source": [
    "## Prepare data for deep learning models...\n",
    "\n",
    "We've to convert our data into a shape that is suitable for deep learning models.\n",
    "First we'll tokenize our data and then embbed it and finally feed into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2fc8a4c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.338269273494777"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's find out the average sentence length\n",
    "sent_len = [len(sentence.split()) for sentence in train_sentences]\n",
    "avg_len = sum(sent_len)/len(train_sentences)\n",
    "avg_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f5dabb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the length for output_sequence\n",
    "output_seq_len = int(np.percentile(sent_len,95))\n",
    "output_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "614a8199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-19 22:56:33.048032: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-08-19 22:56:33.048186: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2022-08-19 22:56:33.765214: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-08-19 22:56:33.816427: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "# max tokens...\n",
    "max_tokens = 68000\n",
    "\n",
    "# make first tokenizer layer\n",
    "text_vectorizer = TextVectorization(max_tokens=max_tokens,\n",
    "                                   output_sequence_length = output_seq_len,)\n",
    "\n",
    "# adapt it to the train data...\n",
    "text_vectorizer.adapt(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9d227a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence : clinicaltrials.gov nct@ http://clinicaltrials.gov/ct@/show/nct@ .\n",
      "\n",
      "Vectorizer Sentence : [[  275   176 11581     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "# check out the vectorizer\n",
    "random_sentence = random.choice(train_sentences)\n",
    "print(f\"Original Sentence : {random_sentence}\\n\")\n",
    "print(f\"Vectorizer Sentence : {text_vectorizer([random_sentence])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "231e98f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Words in the Vocabulary : 64841\n",
      "\n",
      "Top five words :\n",
      " ['', '[UNK]', 'the', 'and', 'of']\n",
      "\n",
      "Bottom five words : \n",
      " ['aainduced', 'aaigroup', 'aachener', 'aachen', 'aaacp']\n"
     ]
    }
   ],
   "source": [
    "# find out the vocabulary from data...\n",
    "vocab = text_vectorizer.get_vocabulary()\n",
    "print(f\"Total Words in the Vocabulary : {len(vocab)}\\n\")\n",
    "print(f\"Top five words :\\n {vocab[:5]}\\n\")\n",
    "print(f\"Bottom five words : \\n {vocab[-5:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75aaa4e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a5107aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence : \n",
      "this is particularly valuable in situations involving complex visual subject matters , typical in clinical practice .\n",
      "\n",
      "Embedd Sentence : \n",
      " [[[ 0.01854812  0.03406577  0.03569676 ...  0.04980786 -0.01408248\n",
      "    0.01847013]\n",
      "  [-0.00635862  0.03439102 -0.04105362 ...  0.04345198  0.00032137\n",
      "    0.00385853]\n",
      "  [ 0.02368069  0.02760288 -0.0249552  ... -0.0257163   0.04944159\n",
      "   -0.04412895]\n",
      "  ...\n",
      "  [ 0.02198042 -0.02677277 -0.04756169 ...  0.04956089 -0.01161468\n",
      "   -0.0484657 ]\n",
      "  [ 0.02198042 -0.02677277 -0.04756169 ...  0.04956089 -0.01161468\n",
      "   -0.0484657 ]\n",
      "  [ 0.02198042 -0.02677277 -0.04756169 ...  0.04956089 -0.01161468\n",
      "   -0.0484657 ]]]\n"
     ]
    }
   ],
   "source": [
    "# Create Embedd Layer for richer representation for our token numbers...\n",
    "embed_layer = Embedding(input_dim = len(vocab),\n",
    "                       output_dim =128,\n",
    "                       embeddings_initializer=\"uniform\",\n",
    "                       input_length=55,\n",
    "                       name=\"Default_Embedding\")\n",
    "\n",
    "# check out the Embedding layer\n",
    "random_sentence = random.choice(train_sentences)\n",
    "print(f\"Original Sentence : \\n{random_sentence}\\n\")\n",
    "print(f\"Embedd Sentence : \\n {embed_layer(text_vectorizer([random_sentence]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5138d97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset as fast as possible \n",
    "# let's convert our data into tensors...\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_sentences,train_labels))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_sentences,val_labels))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_sentences,test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ccf22958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now convert them into the batches and prefetch for fast load on GPU\n",
    "train_dataset = train_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "da3839be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check out the train dataset\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56d64e0",
   "metadata": {},
   "source": [
    "## Model 1 : Cov1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "992e7ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Conv1D_Model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization (TextVec  (None, 55)               0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " Default_Embedding (Embeddin  (None, 55, 128)          8299648   \n",
      " g)                                                              \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 55, 64)            41024     \n",
      "                                                                 \n",
      " global_average_pooling1d_1   (None, 64)               0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " dense (Dense)               (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,340,997\n",
      "Trainable params: 8,340,997\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# the first model we're going to build is 1-dimensional Convolutional Neural Network...\n",
    "\n",
    "# create input layer\n",
    "inputs = layers.Input(shape = (1,),dtype=tf.string)\n",
    "\n",
    "# text vectorization layer...\n",
    "x = text_vectorizer(inputs)\n",
    "\n",
    "# Embedd layer...\n",
    "x = embed_layer(x)\n",
    "\n",
    "# conv1d layer\n",
    "x = layers.Conv1D(64,kernel_size=5,padding=\"same\",activation=\"relu\")(x)\n",
    "\n",
    "# add the average pool layer....\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "# the output layer\n",
    "outputs = layers.Dense(num_classes,activation=\"softmax\")(x)\n",
    "\n",
    "# build the model...\n",
    "model_1 = tf.keras.Model(inputs,outputs,name=\"Conv1D_Model\")\n",
    "\n",
    "# compile the model\n",
    "model_1.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "               optimizer = tf.keras.optimizers.Adam(),\n",
    "               metrics = [\"accuracy\"])\n",
    "# model_1 summary\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5b027400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "  1/562 [..............................] - ETA: 4:01 - loss: 1.6093 - accuracy: 0.0625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-19 22:58:36.598745: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "561/562 [============================>.] - ETA: 0s - loss: 0.9177 - accuracy: 0.6369"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-19 22:58:55.404192: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "562/562 [==============================] - 19s 34ms/step - loss: 0.9169 - accuracy: 0.6373 - val_loss: 0.6828 - val_accuracy: 0.7377\n",
      "Epoch 2/3\n",
      "562/562 [==============================] - 18s 32ms/step - loss: 0.6540 - accuracy: 0.7581 - val_loss: 0.6290 - val_accuracy: 0.7699\n",
      "Epoch 3/3\n",
      "562/562 [==============================] - 18s 32ms/step - loss: 0.6144 - accuracy: 0.7741 - val_loss: 0.5945 - val_accuracy: 0.7866\n"
     ]
    }
   ],
   "source": [
    "# fitting the model to data.....\n",
    "history_1 = model_1.fit(train_dataset,\n",
    "                        steps_per_epoch=int(0.1 * len(train_dataset)),\n",
    "                       epochs=3,\n",
    "                       validation_data=val_dataset,\n",
    "                       validation_steps = int(0.1 * len(val_dataset)),\n",
    "                       callbacks =[tf.keras.callbacks.EarlyStopping(patience=2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "209e437d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 67/945 [=>............................] - ETA: 2s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-19 22:59:31.585312: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "945/945 [==============================] - 3s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[4.2674640e-01, 1.4814913e-01, 6.9188677e-02, 3.2962820e-01,\n",
       "        2.6287546e-02],\n",
       "       [4.8393324e-01, 2.2787191e-01, 1.2383969e-02, 2.6750058e-01,\n",
       "        8.3103646e-03],\n",
       "       [1.5913843e-01, 4.6922504e-03, 1.3429127e-03, 8.3478421e-01,\n",
       "        4.2152493e-05],\n",
       "       ...,\n",
       "       [4.7417516e-06, 7.2817388e-04, 7.2644791e-04, 4.8627176e-06,\n",
       "        9.9853575e-01],\n",
       "       [5.6012433e-02, 4.9329013e-01, 8.7086372e-02, 5.7346497e-02,\n",
       "        3.0626446e-01],\n",
       "       [1.7591362e-01, 6.8200552e-01, 6.2432874e-02, 3.9306335e-02,\n",
       "        4.0341672e-02]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# time to make prediction with our model\n",
    "model_1_probs = model_1.predict(val_dataset)\n",
    "model_1_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "be86be4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(30212,), dtype=int64, numpy=array([0, 0, 3, ..., 4, 1, 1])>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert proeb probs into labels\n",
    "model_1_preds = tf.argmax(model_1_probs,axis=1)\n",
    "model_1_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d1b17c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Accuracy': 78.7733350986363,\n",
       " 'Precision': 0.7840981186048689,\n",
       " 'Recall': 0.787733350986363,\n",
       " 'Fscore': 0.7853102395179401}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate our model performance....\n",
    "model_1_results = calculate_results(val_labels,model_1_preds)\n",
    "model_1_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8f6121",
   "metadata": {},
   "source": [
    "## Model 2 : Conv1D with Character Embeddings..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c5588aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a function that will split sentences into characters...\n",
    "def split_sentences(sentence):\n",
    "    return \" \".join(list(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "83dcb8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t o   i n v e s t i g a t e   t h e   e f f i c a c y   o f   @   w e e k s   o f   d a i l y   l o w - d o s e   o r a l   p r e d n i s o l o n e   i n   i m p r o v i n g   p a i n   ,   m o b i l i t y   ,   a n d   s y s t e m i c   l o w - g r a d e   i n f l a m m a t i o n   i n   t h e   s h o r t   t e r m   a n d   w h e t h e r   t h e   e f f e c t   w o u l d   b e   s u s t a i n e d   a t   @   w e e k s   i n   o l d e r   a d u l t s   w i t h   m o d e r a t e   t o   s e v e r e   k n e e   o s t e o a r t h r i t i s   (   o a   )   .\n"
     ]
    }
   ],
   "source": [
    "# split sentences into characters....\n",
    "train_char = [split_sentences(sentence) for sentence in train_sentences]\n",
    "val_char = [split_sentences(sentence) for sentence in val_sentences]\n",
    "test_char = [split_sentences(sentence) for sentence in test_sentences]\n",
    "\n",
    "print(train_char[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f699c57d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149.3662574983337"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# average character length....\n",
    "char_len = [len(sentence) for sentence in train_sentences]\n",
    "mean_char = np.mean(char_len)\n",
    "mean_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d8c13c38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_seq_len = int(np.percentile(char_len,95))\n",
    "output_seq_len "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "379eab20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-19 22:59:36.871365: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "# create char-level vectorization...\n",
    "alp = string.ascii_lowercase + string.digits + string.printable\n",
    "\n",
    "num_tokens= len(alp) + 2 # for space and OOV objects\n",
    "char_token = TextVectorization(max_tokens = num_tokens,\n",
    "                              output_sequence_length = output_seq_len,\n",
    "                              standardize = \"lower_and_strip_punctuation\",\n",
    "                              name= \"Char_Vectorization\")\n",
    "\n",
    "# adapt to data...\n",
    "char_token.adapt(train_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "83e9fb5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text : \n",
      " t h i s   p r o t o c o l   f o r   a   r a n d o m i z e d   c o n t r o l l e d   s t u d y   i n v e s t i g a t i n g   t h e   i m p a c t   o f   t h e   t i m i n g   o f   r e n a l   r e p l a c e m e n t   t h e r a p y   i n i t i a t i o n   s h o u l d   p r o v i d e   a n   a n s w e r   t o   a   k e y   q u e s t i o n   f o r   t h e   m a n a g e m e n t   o f   p a t i e n t s   w i t h   a c u t e   k i d n e y   i n j u r y   i n   t h e   c o n t e x t   o f   s e p t i c   s h o c k   ,   f o r   w h o m   t h e   m o r t a l i t y   r a t e   r e m a i n s   c l o s e   t o   @   %   d e s p i t e   i m p r o v e d   u n d e r s t a n d i n g   o f   p h y s i o p a t h o l o g y   a n d   r e c e n t   t h e r a p e u t i c   a d v a n c e s   .\n",
      "\n",
      "Vectorizer Text : \n",
      "[[ 3 13  4  9 14  8  7  3  7 11  7 12 17  7  8  5  8  5  6 10  7 15  4 25\n",
      "   2 10 11  7  6  3  8  7 12 12  2 10  9  3 16 10 19  4  6 21  2  9  3  4\n",
      "  18  5  3  4  6 18  3 13  2  4 15 14  5 11  3  7 17  3 13  2  3  4 15  4\n",
      "   6 18  7 17  8  2  6  5 12  8  2 14 12  5 11  2 15  2  6  3  3 13  2  8\n",
      "   5 14 19  4  6  4  3  4  5  3  4  7  6  9 13  7 16 12 10 14  8  7 21  4\n",
      "  10  2  5  6  5  6  9 20  2  8  3  7  5 23  2 19 26 16  2  9  3  4  7  6\n",
      "  17  7  8  3 13  2 15  5  6  5 18  2 15  2  6  3  7 17 14  5  3  4  2  6\n",
      "   3  9 20  4  3 13  5 11 16  3  2 23  4 10  6  2 19  4  6 27 16  8 19  4\n",
      "   6  3 13  2 11  7  6  3  2 24  3  7 17  9  2 14  3  4 11  9 13  7 11 23\n",
      "  17  7  8 20 13  7 15  3 13  2 15  7  8  3  5 12  4  3 19  8  5  3  2  8\n",
      "   2 15  5  4  6  9 11 12  7  9  2  3  7 10  2  9 14  4  3  2  4 15 14  8\n",
      "   7 21  2 10 16  6 10  2  8  9  3  5  6 10  4  6 18  7 17 14 13 19  9  4\n",
      "   7 14]]\n"
     ]
    }
   ],
   "source": [
    "# let's check it out....\n",
    "random_char = random.choice(train_char)\n",
    "print(f\"Original Text : \\n {random_char}\\n\")\n",
    "print(f\"Vectorizer Text : \\n{char_token([random_char])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0e668ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total different characters in vocab : 28\n",
      "\n",
      "Top 5 chars : ['', '[UNK]', 'e', 't', 'i']\n",
      "\n",
      "Bottom 5 chars : ['k', 'x', 'z', 'q', 'j']\n"
     ]
    }
   ],
   "source": [
    "# find out the vocabulary in the data.....\n",
    "vocab_char = char_token.get_vocabulary()\n",
    "print(f\"Total different characters in vocab : {len(vocab_char)}\\n\")\n",
    "print(f\"Top 5 chars : {vocab_char[:5]}\\n\")\n",
    "print(f\"Bottom 5 chars : {vocab_char[-5:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d13b93",
   "metadata": {},
   "source": [
    "### Character level embedding..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "373bd676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text : \n",
      " v i s u a l   a t l a s   a n d   c o n s e n s u s   t r e a t m e n t   g u i d e l i n e   u s a g e   i n   t h e   d e v e l o p m e n t   o f   r e c t a l   c a n c e r   i m r t   t r e a t m e n t   p l a n s   r e d u c e d   t h e   i n t e r - o b s e r v e r   r a d i o b i o l o g i c a l   v a r i a t i o n   ,   w i t h   c l i n i c a l l y   r e l e v a n t   t c p   a l t e r a t i o n   f o r   c t v   a n d   p t v   v o l u m e s   .\n",
      "\n",
      "Vectorizer Text : \n",
      "[[[ 0.02302421  0.04897788 -0.01751237 ... -0.03485013  0.02618638\n",
      "   -0.02471949]\n",
      "  [ 0.04320708  0.00625777 -0.00241963 ... -0.04911428 -0.00754761\n",
      "   -0.04169189]\n",
      "  [ 0.03426358  0.04542596  0.04011219 ...  0.00630441 -0.04588494\n",
      "    0.02532015]\n",
      "  ...\n",
      "  [-0.0188207   0.03263413  0.01849456 ... -0.03177612  0.04988373\n",
      "   -0.01304941]\n",
      "  [-0.0188207   0.03263413  0.01849456 ... -0.03177612  0.04988373\n",
      "   -0.01304941]\n",
      "  [-0.0188207   0.03263413  0.01849456 ... -0.03177612  0.04988373\n",
      "   -0.01304941]]]\n"
     ]
    }
   ],
   "source": [
    "char_embedd = Embedding(input_dim = num_tokens,\n",
    "                       output_dim = 25,\n",
    "                       mask_zero = False,\n",
    "                       name= \"char_Embed\")\n",
    "\n",
    "# let'check it out...\n",
    "random_char = random.choice(train_char)\n",
    "print(f\"Original Text : \\n {random_char}\\n\")\n",
    "print(f\"Vectorizer Text : \\n{char_embedd(char_token([random_char]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7bf4c183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's build the char-level Conv1D Model....\n",
    "inputs = layers.Input(shape=(1,),dtype=\"string\")\n",
    "x = char_token(inputs) # passing the input to char vectorizer\n",
    "\n",
    "# passing the char vectorizer to embedding...\n",
    "x = char_embedd(x)\n",
    "\n",
    "# passing the embedding to conv layer\n",
    "x = layers.Conv1D(64,kernel_size=5,activation=\"relu\",padding=\"same\")(x)\n",
    "\n",
    "# passing the conv layer to pool layer\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "# outputs\n",
    "outputs = layers.Dense(num_classes,activation=\"softmax\")(x)\n",
    "\n",
    "# model building...\n",
    "model_2 = tf.keras.Model(inputs,outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "adb246d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model_2.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "               optimizer = tf.keras.optimizers.Adam(),\n",
    "               metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d8a6bd3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " Char_Vectorization (TextVec  (None, 290)              0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " char_Embed (Embedding)      (None, 290, 25)           3450      \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 290, 64)           8064      \n",
      "                                                                 \n",
      " global_average_pooling1d_2   (None, 64)               0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,839\n",
      "Trainable params: 11,839\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model summary\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "deade06d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create char tensor data-sets..\n",
    "train_char_dataset = tf.data.Dataset.from_tensor_slices((train_char,train_labels)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "val_char_dataset = tf.data.Dataset.from_tensor_slices((val_char,val_labels)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "train_char_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9a005665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "    3/18004 [..............................] - ETA: 9:32 - loss: 1.6080 - accuracy: 0.2083   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-19 22:59:41.731329: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5627/18004 [========>.....................] - ETA: 5:44 - loss: 1.3074 - accuracy: 0.4516WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 54012 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-19 23:02:18.738406: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 3021 batches). You may need to use the repeat() function when building your dataset.\n",
      "18004/18004 [==============================] - 162s 9ms/step - loss: 1.3074 - accuracy: 0.4516 - val_loss: 1.2403 - val_accuracy: 0.4883\n"
     ]
    }
   ],
   "source": [
    "# fitting the model to data...\n",
    "history_2 = model_2.fit(train_char_dataset,\n",
    "                       epochs =3,\n",
    "                       steps_per_epoch = int(0.1 * len(train_char)),\n",
    "                       validation_data = val_char_dataset,\n",
    "                       validation_steps = int(0.1 * len(val_char)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ee6eace0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 47/945 [>.............................] - ETA: 3s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-19 23:02:23.274929: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "945/945 [==============================] - 5s 5ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Accuracy': 48.82828015358136,\n",
       " 'Precision': 0.4444318962929611,\n",
       " 'Recall': 0.4882828015358136,\n",
       " 'Fscore': 0.44412241465253566}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's make prediction and calculate the results...\n",
    "model_2_results = calculate_results(val_labels,tf.argmax(model_2.predict(val_char_dataset),axis=1))\n",
    "model_2_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dd2f3679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " Char_Vectorization (TextVec  (None, 290)              0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " char_Embed (Embedding)      (None, 290, 25)           3450      \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 290, 64)           8064      \n",
      "                                                                 \n",
      " global_average_pooling1d_2   (None, 64)               0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,839\n",
      "Trainable params: 11,839\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "65369b78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Fscore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BaseLine</th>\n",
       "      <td>72.183238</td>\n",
       "      <td>0.718647</td>\n",
       "      <td>0.721832</td>\n",
       "      <td>0.698925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Conv1D</th>\n",
       "      <td>78.773335</td>\n",
       "      <td>0.784098</td>\n",
       "      <td>0.787733</td>\n",
       "      <td>0.785310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Char_Model</th>\n",
       "      <td>48.828280</td>\n",
       "      <td>0.444432</td>\n",
       "      <td>0.488283</td>\n",
       "      <td>0.444122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Accuracy  Precision    Recall    Fscore\n",
       "BaseLine    72.183238   0.718647  0.721832  0.698925\n",
       "Conv1D      78.773335   0.784098  0.787733  0.785310\n",
       "Char_Model  48.828280   0.444432  0.488283  0.444122"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare results....\n",
    "all_model = pd.DataFrame({\n",
    "    \"BaseLine\" : baseline_results,\n",
    "    \"Conv1D\" : model_1_results,\n",
    "    \"Char_Model\" : model_2_results\n",
    "})\n",
    "\n",
    "all_model = all_model.transpose()\n",
    "all_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cb9854cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot and compare all of the model results....\n",
    "all_model.plot(kind=\"bar\",figsize=(10,7)).legend(bbox_to_anchor=(1.0,1.0));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "db48211e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "942/942 [==============================] - 5s 5ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(30135,), dtype=int64, numpy=array([4, 4, 4, ..., 4, 4, 4])>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make predictions with char model...\n",
    "preds = tf.argmax(model_2.predict(test_dataset),axis=1)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2773750f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Accuracy': 32.407499585199936,\n",
       " 'Precision': 0.2803063236555915,\n",
       " 'Recall': 0.32407499585199934,\n",
       " 'Fscore': 0.1629559788998415}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = calculate_results(test_labels,preds)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5862a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
